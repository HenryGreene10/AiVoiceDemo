<!doctype html>
<html lang="en">
<head>
  <!-- ===== Page metadata (keep title for SEO; the reader uses <h1>) ===== -->
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Where should archaeologists dig next?</title>
  <meta name="description" content="OpenAI contest winners used machine learning to predict new archaeological sites in the Amazon." />
  <meta name="author" content="National Geographic Staff" />

  <!-- ===== Player styles (serve these from your root) ===== -->
  <link rel="stylesheet" href="/mini-player.css?v=108" />
  <link rel="stylesheet" href="/pro-player.css?v=108" />

  <!-- ===== Page look + icon variables for mini-player (optional) ===== -->
  <style>
    :root{
      /* custom icons for the mini-player (optional) */
      --mp-icon: 20px;
      --icon-play:  url('/icons/my-play.svg');
      --icon-pause: url('/icons/my-pause.svg');
      --icon-back:  url('/icons/my-back.svg');
      --icon-fwd:   url('/icons/my-fwd.svg');
    }
    body{
      font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif;
      margin: 0; padding: 24px; background: #ffffff;
    }
    main{ max-width: 720px; margin: 0 auto; }
  </style>
  <script>
    /* Enables the CSS mask-icon mode in mini-player.css */
    document.documentElement.classList.add('custom-icons');
  </script>
</head>







<script>
  (function(){
    const BASE   = "https://aivoicedemo.onrender.com";
    const VOICE  = "21m00Tcm4TlvDq8ikWAM";
    const TENANT = "demo123";
    const SEL    = "#demo-article";
  
    // --- helpers ---
    function firstWords(str, n=5){
      const words = (str || "").trim().split(/\s+/);
      return words.length <= n ? (str || "") : words.slice(0,n).join(" ") + "…";
    }
    // 13,000 -> thirteen thousand (simple US English cardinals up to billions)
    function numberToWordsUS(num){
      const ones=["zero","one","two","three","four","five","six","seven","eight","nine"];
      const teens=["ten","eleven","twelve","thirteen","fourteen","fifteen","sixteen","seventeen","eighteen","nineteen"];
      const tens=["","","twenty","thirty","forty","fifty","sixty","seventy","eighty","ninety"];
      function chunk(n){
        let s=""; const h=Math.floor(n/100), t=n%100;
        if (h) s += ones[h]+" hundred";
        if (t){ if (s) s+=" ";
          if (t<10) s+=ones[t];
          else if (t<20) s+=teens[t-10];
          else { s+=tens[Math.floor(t/10)]; if (t%10) s+="-"+ones[t%10]; }
        }
        return s || "zero";
      }
      if (num===0) return "zero";
      const units=[""," thousand"," million"," billion"]; const parts=[];
      for (let i=0; num>0 && i<units.length; i++){ const c=num%1000; if (c) parts.unshift(chunk(c)+units[i]); num=Math.floor(num/1000); }
      return parts.join(" ");
    }
    function normalizeNumbers(text){
      return text.replace(/\b\d{1,3}(?:,\d{3})+\b|\b\d{4,9}\b/g, m => {
        const n = Number(m.replace(/,/g,""));
        return Number.isFinite(n) ? numberToWordsUS(n) : m;
      });
    }
  
    async function ensureMini(){
      if (window.AiMini) return;
      await new Promise((resolve, reject)=>{
        const s=document.createElement("script");
        s.src="/mini-player.js?v=108";
        s.onload=resolve; s.onerror=reject;
        document.head.appendChild(s);
      });
    }
  
    function buildText(){
      const h1 = (document.querySelector("h1")?.innerText || "AI Listen").trim();
      const dek = (document.querySelector(".dek,.subtitle,h2")?.innerText || "").trim();
      const by  = (document.querySelector(".byline")?.textContent || "").replace(/^By\s*/i,"").trim();
  
      let body = "";
      (document.querySelector(SEL) || document.body)
        .querySelectorAll("p,li,blockquote")
        .forEach(n => { const s=(n.innerText||"").replace(/\s+/g," ").trim(); if (s) body+=s+" "; });
  
      if (body.toLowerCase().startsWith(h1.toLowerCase())) body = body.slice(h1.length).trim();
      body = body.replace(/^\s*By\s+.+?\s{2,}/i,"");
  
      const plain = [h1, dek, by ? `By ${by}` : "", body].filter(Boolean).join("\n\n").slice(0, 12000);
      return { h1, dek, by, plain };
    }
  
    let busy = false;
    async function play(){
      if (busy) return;
      busy = true;
      try{
        await ensureMini();
  
        const { h1, dek, plain } = buildText();
        const pageTitle = h1 || "AI Listen";
        const subRaw    = dek || (document.querySelector(".byline")?.textContent?.trim()) || location.hostname;
  
        // Open mini immediately with truncated title
        window.AiMini.open({ title: firstWords(pageTitle, 5), subtitle: firstWords(subRaw, 5) });
        // Tooltip with full title
        setTimeout(()=>{ const t=document.querySelector("#ai-mini .ai-title"); if (t) t.title = pageTitle; }, 0);
        // Bind timebar labels
        window.AIL_bindTimebar && window.AIL_bindTimebar();
  
        // Request TTS (use number-normalized text)
        const spoken = normalizeNumbers(plain);
        const r = await fetch(BASE + "/api/tts", {
          method: "POST",
          headers: { "content-type": "application/json", "x-tenant-key": TENANT },
          body: JSON.stringify({ text: spoken, voice_id: VOICE, preset: "news" })
        });
        const raw = await r.text();
        if (!r.ok) throw new Error(`TTS ${r.status}: ${raw}`);
        const j = JSON.parse(raw);
        let url = j.audioUrl || j.audio_url || j.url;
        if (url && !/^https?:\/\//i.test(url)) url = BASE + url;
  
        const a = window.AiMini.audio?.() || document.getElementById("ai-listen-audio") || (()=> {
          const el=document.createElement("audio"); el.id="ai-listen-audio"; document.body.appendChild(el); return el;
        })();
        try{ a.pause(); a.removeAttribute("src"); a.currentTime = 0; }catch{}
        a.src = url; a.preload = "auto";
        a.addEventListener("canplay", ()=> a.play().catch(()=>{}), { once:true });
        a.load();
      } catch(e){
        console.error("[AIL hotfix] play failed", e);
        alert("Playback error (see console).");
      } finally {
        busy = false;
      }
    }
  
    // Bind to your visible button (and stop other handlers from double-firing)
    document.addEventListener("click", (e)=>{
      const btn = e.target.closest("#ai-listen-btn");
      if (!btn) return;
      e.preventDefault();
      e.stopImmediatePropagation();
      play();
    });
  })();
  </script>
  
  <script>
  (function(){
    function fmt(t){ if(!isFinite(t)||t<0) return "0:00";
      const m=Math.floor(t/60), s=Math.floor(t%60);
      return m+":"+(s<10?"0"+s:s);
    }

    function bindWhenReady(){
      const mini = document.getElementById("ai-mini");
      if (!mini) return;

      // wait for the meter row to exist (.ai-meter or .ai-progress)
      const target = mini.querySelector(".ai-meter, .ai-progress");
      if (target) return bind(target);

      const mo = new MutationObserver(() => {
        const t = mini.querySelector(".ai-meter, .ai-progress");
        if (t){ mo.disconnect(); bind(t); }
      });
      mo.observe(mini, { childList:true, subtree:true });
    }

    function bind(target){
      // create row if missing
      let row = target.parentElement.querySelector(".ai-timebar");
      if (!row){
        row = document.createElement("div");
        row.className = "ai-timebar";
        row.innerHTML = `<span data-ai-elapsed>0:00</span><span data-ai-total>0:00</span>`;
        // place right after the meter/progress
        (target.parentElement || target).appendChild(row);
      }
      const el  = row.querySelector("[data-ai-elapsed]");
      const tot = row.querySelector("[data-ai-total]");

      const a = (window.AiMini && window.AiMini.audio && window.AiMini.audio())
             || document.getElementById("ai-listen-audio");
      if (!a) return;

      const updTot = () => { if (tot) tot.textContent = fmt(a.duration); };
      const updCur = () => { if (el)  el.textContent  = fmt(a.currentTime); };
      a.addEventListener("loadedmetadata", updTot);
      a.addEventListener("durationchange",  updTot);
      a.addEventListener("timeupdate",      updCur);
      a.addEventListener("ended",           updCur);
      updTot(); updCur();
    }

    // call this anytime you open the mini
    window.AIL_bindTimebar = function(){
      // wait until #ai-mini exists, then bind
      if (document.getElementById("ai-mini")) bindWhenReady();
      else {
        const mo = new MutationObserver(()=>{
          if (document.getElementById("ai-mini")){ mo.disconnect(); bindWhenReady(); }
        });
        mo.observe(document.body, { childList:true, subtree:true });
      }
    };
  })();
  </script>
    
    <script>
      (function(){
        function attach(mini){
          const root = document.documentElement;
      
          function setPad(){
            // Measure the card and add some breathing room
            const h = mini.offsetHeight || 120;
            root.style.setProperty('--ai-mini-pad', (h + 16) + 'px');
            root.classList.add('ai-mini-open');
          }
          function clearPad(){
            root.classList.remove('ai-mini-open');
          }
      
          // Watch the mini for visibility changes
          const mo = new MutationObserver(() => {
            if (mini.classList.contains('show')) setPad(); else clearPad();
          });
          mo.observe(mini, { attributes: true, attributeFilter: ['class','style'] });
      
          // Recompute on resize/rotation
          window.addEventListener('resize', () => {
            if (mini.classList.contains('show')) setPad();
          });
      
          // If it started visible, pad now
          if (mini.classList.contains('show')) setPad();
        }
      
        // Wait for #ai-mini to exist (widget inserts it)
        function waitForMini(){
          const mini = document.getElementById('ai-mini');
          if (mini) return attach(mini);
          const obs = new MutationObserver(() => {
            const el = document.getElementById('ai-mini');
            if (el){ obs.disconnect(); attach(el); }
          });
          obs.observe(document.body, { childList: true, subtree: true });
        }
      
        if (document.readyState !== 'loading') waitForMini();
        else document.addEventListener('DOMContentLoaded', waitForMini);
      })();
      </script>

  
  



<body>
  <!-- ===== Article content (the reader pulls text from #demo-article) ===== -->
  <main class="article">
    <header class="article-header">
      <!-- H1 is what the mini-player shows as title -->
      <h1>Where should archaeologists dig next? The winners of this OpenAI contest can tell them.</h1>
      
      <!-- Optional subheading/dek (read before body with a short pause) -->
      <p class="dek">Machine learning points to dozens of likely archaeological sites hidden across the Amazon rainforest.</p>

      <!-- Byline (the reader says “By …” once; we strip duplicates from body) -->
      <div class="byline">By National Geographic Staff</div>
      <br>
      <button id="ai-listen-btn" class="listen-btn">Listen</button>
      <!-- The widget (v108+) will insert the Listen pill right under the H1 -->
    </header>

    

    <!-- Paste ONLY readable story text here: paragraphs, lists, blockquotes. -->
    <!-- You can omit images, captions, embeds, and footnotes (the widget strips them anyway). -->
    <article id="demo-article" class="article-body">
      <!-- Replace these sample paragraphs with the NatGeo article text -->
      <p>Buried deep within millions of square miles of dense Amazon rainforest may lie tens of thousands of hidden archeological sites, where stone tools and rock paintings 
        are testaments to civilizations that existed as far back as 13,000 years ago.</p>
      <p>But the thick forest sprawled across nine countries and home to hundreds of Indigenous groups today, is too vast and often difficult for archeologists to physically 
        survey for hidden sites. Increasingly, researchers are looking to newer technologies like Artificial Intelligence and machine learning for help. </p>
      <p>That's why two archaeologists recently collaborated with OpenAI, the company behind ChatGPT, to judge a public competition encouraging tech enthusiasts
         to explore large troves of satellite images and remote sensor data for signs of undiscovered archeological sites.</p>
      <p>The winning three-person team of the "OpenAI to Z Challenge," announced Thursday, found 67 distinct patches across the Amazon, each measuring about a 
        square mile, that they think could contain historically valuable ancient sites and provide potential starting points for field exploration. 
        The judges included Egyptologist Sarah Parcak and Mesoamerican archaeologist Chris Fisher.</p>
      <p>The winning team, which calls itself “Black Bean,” trained deep learning models on several publicly available datasets, including remote sensing LiDAR data
         and satellite images from Google Earth Engine and NASA's digital elevation models, among others. The team says they then used OpenAI's GPT-4o model to learn
          the pattern of known archeological sites in the Amazon rainforest and compare them to unexplored swaths of the Amazon, mainly in Brazil.
           It then highlighted dozens of coordinates for future exploration.</p>
      <p>Many of the areas they identified appeared to be clustered along bodies of water.
        “Our results actually make sense on a common-sense level,” says Yao Zhao, a member of the winning team who is 
        currently a software engineer at Meta, and was on a career break during the competition to learn more about AI applications. Ancient civilizations, after all, tended to flourish near accessible water sources. 
        Being able to quickly churn through millions of square miles of geographic data within a few weeks could 
        make it easier for archeologists to find patterns without relying on groundwork first, he adds. The first-place winners received a $250,000 cash prize and credits to use premium OpenAI products.</p>
      
      <h2>Adding AI to the archaeologist's toolkit</h2>
      <p>Machine learning isn't a completely new tool to explorers of the ancient world. Parcak, an archeologist at the University of Alabama at Birmingham, has used satellite images, thermal imaging and LiDAR, 
        which employs planes or drones to emit pulses from sensors and observe their return, for a few decades in her exploration of Egypt, Tunisia and other countries.</p>
      <p>Those imaging tools and techniques, combined with machine learning trained to find patterns in data, have helped her uncover thousands of previously unknown settlements and tombs within 
        known archeological sites. But newer AI models could build on those uses by looking beyond established archeological targets, turning up entirely new areas for archeologists to investigate, she says. </p>
      <p>Many sites are disappearing across the world as sea levels rise, vegetation spreads or recedes, and as humans build and migrate, abandoning or destroying valuable historical records.</p>
<!-- ...more <p>…</p> blocks as needed ... -->
    </article>
  </main>

  <!-- ===== Scripts =====
       You do NOT need any button-mover or SSML scripts.
       The widget loads the mini-player and places the Listen pill for you.
       Attach any ElevenLabs pronunciation dictionary to the same voice you use below. -->
  <script async
       src="/tts-widget.v1.js?v=108"
       data-base="https://aivoicedemo.onrender.com"
       data-voice="21m00Tcm4TlvDq8ikWAM"
       data-tenant="demo123"
       data-selector="#demo-article">
    </script>

  <!-- (Optional) If you also ship a pronunciation dictionary file in ElevenLabs,
       you don't need to change this HTML: just attach that dictionary to the voice
       used in data-voice inside the ElevenLabs dashboard. -->
</body>
</html>
